% math_framework.tex

\section{Mathematical Framework}
\label{sec:math_framework}

In this section, we develop the mathematical foundation that connects neural networks to the Mahalanobis distance, thereby providing a framework for interpreting neural network operations through the lens of statistical distance metrics. We begin by revisiting key concepts related to Gaussian distributions and the Mahalanobis distance, followed by a detailed exploration of how neural network components, particularly linear layers and activation functions, can approximate these distance metrics. This framework not only enhances our understanding of neural network behavior but also lays the groundwork for leveraging statistical principles to improve network interpretability and training dynamics.

\subsection{Mahalanobis Distance for a Multivariate Gaussian Distribution}

A multivariate Gaussian (normal) distribution is a fundamental concept in statistics, describing a \(d\)-dimensional random vector \(\mathbf{x} \in \mathbb{R}^d\) with a mean vector \(\boldsymbol{\mu} \in \mathbb{R}^d\) and a covariance matrix \(\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}\) \citep{bishop2006pattern}. We denote this distribution as \(\mathbf{x} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})\).

The Mahalanobis distance quantifies the distance between a point \(\mathbf{x}\) and the mean \(\boldsymbol{\mu}\) of a distribution, while considering the covariance structure of the data \citep{mahalanobis1936generalized, demaesschalck2000mahalanobis}. It is defined as:

\begin{equation}
\label{eq:mahalanobis_distance}
D_M(\mathbf{x}) = \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) }.
\end{equation}

This metric adjusts for variance across dimensions by effectively whitening the data, resulting in a spherical distance measure.

\subsection{Principal Component Analysis (PCA)}

Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a new coordinate system, emphasizing directions (principal components) that capture the most variance \citep{jolliffe2002principal}. When performing PCA on the covariance matrix $\boldsymbol{\Sigma}$, it is decomposed using eigenvalue decomposition:

\begin{equation}
    \label{eq:pca_decomposition}
    \boldsymbol{\Sigma} = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top,
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_d]$ is a matrix whose columns are the orthogonal unit eigenvectors of $\boldsymbol{\Sigma}$.
    \item $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_d)$ is a diagonal matrix of the corresponding eigenvalues $\lambda_i$, representing the variance along each principal component.
\end{itemize}

Substituting $\mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top$ for $\boldsymbol{\Sigma}$ in the Mahalanobis distance equation \eqref{eq:mahalanobis_distance}, we obtain:

\begin{equation}
    \label{eq:mahalanobis_pca}
    D_M(\mathbf{x}) = \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{V} \boldsymbol{\Lambda}^{-1} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}) }.
\end{equation}

To further simplify, we can express the Mahalanobis distance in terms of the principal components:

\begin{align}
D_M(\mathbf{x}) &= \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{V} \boldsymbol{\Lambda}^{-1} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}) } \nonumber \\
&= \sqrt{ (\mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}))^\top \boldsymbol{\Lambda}^{-1} (\mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu})) } \nonumber \\
&= \sqrt{ \sum_{i=1}^{d} \lambda_i^{-1} \left( \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right)^2 } \nonumber \\
&= \left\| \lambda_i^{-1/2} \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right\|_2.
\label{eq:mahalanobis_pca_l2}
\end{align}

where $\| \cdot \|_2$ denotes the Euclidean (L2) norm. 

This shows that the Mahalanobis distance can also be expressed as the L2 norm of the number of standard deviations of \(\mathbf{x}\) along each principal component.

\subsection{Connecting Neural Networks to Mahalanobis Distance}

We consider the Mahalanobis distance along a single principal component.

\begin{equation}
    \label{eq:mahalanobis_single_component}
    D_{M,i}(\mathbf{x}) = \left| \lambda_i^{-1/2} \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right|,
\end{equation}

This equation projecting the centered data $(\mathbf{x} - \boldsymbol{\mu})$ onto the direction of variance defined by the principal component eigenvector and scales by the inverse square root of the eigenvalue (variance).

Let 
\begin{align}
    \mathbf{W} &= \lambda_i^{-1/2} \mathbf{v}_i^\top, \\
    \mathbf{b} &= - \lambda_i^{-1/2} \mathbf{v}_i^\top \boldsymbol{\mu}.
\end{align}

We can simplify Equation \eqref{eq:mahalanobis_single_component} to

\begin{equation}
    \label{eq:mahalanobis_linear}
    D_{M,i}(\mathbf{x}) = \left| \mathbf{W} \mathbf{x} - \mathbf{b} \right|,
\end{equation}

This is identical to the equation for a linear layer where $\boldsymbol{W}$ represents the weight matrix, $\boldsymbol{b}$ the bias vector, and the Abs function serves as the activation function.

Linear nodes with Abs activations can be interpreted as 1d Gaussians along a direction of variance with the decision boundary passing through the mean of the modelled cluster.

Extending this to all principal components, the Mahalanobis distance can be represented as a multiple linear nodes, each corresponding to a principal component, followed by an Abs activation. This layered structure inherently accounts for the covariance structure of the data, effectively 'whitening' the input.

\subsection{Non-Uniqueness of Whitening}

Calculating the Mahalanobis distance along each principal component results in a whitened data set. 

Whitening data is defined as follows:

\[
\mathbf{x}_w = \boldsymbol{\Lambda}^{-1/2} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}),
\]

where \(\mathbf{x}_w\) follows a distribution \(\mathcal{N}(0, I)\).

The Mahalanobis distance can then be expressed as:

\[
D_M(\mathbf{x}_w) = \|\mathbf{x}_w\|_2.
\]

However the vectors to generate whitened data are not unique.

We can select a rotation matrix \(\mathbf{R} \in SO(n)\) such that:

\begin{align}
    D_M(\mathbf{x}_w) &= D_M(\mathbf{R} \mathbf{x}_w) \\
                      &= D_M\left( (\mathbf{R} \boldsymbol{\Lambda}^{-1/2} \mathbf{V}^\top) (\mathbf{x} - \boldsymbol{\mu}) \right) \\
                      &= D_M\left( \boldsymbol{\Lambda}^{-1/2} (\mathbf{V}^\top \mathbf{R}) (\mathbf{x} - \boldsymbol{\mu}) \right).
    \end{align}
    
The Mahalanobis distance for whitened data remains invariant under the application of rotation matrices. If we transform the rotated basis back to the orignal space, we end up with a whitening basis whose component vectors may not be orthogonal.

This implies that, although linear nodes can represent principal components, they are unlikely to actually learn the principal components when estimating Mahalanobis distances. Instead, they are expected to learn an basis that effectively whitens the data for use in the Mahalanobis distance estimation. However, the decision boundary of the learned hyperplane should still pass through the data mean.

We can calculate one of these rotated principal components as the weighted sum of principle components with a unity constraint on the weights.

\begin{equation}
    \label{eq:pc_mixture}
    \boldsymbol{W_c} = \sum a_i \mathbf{W_i}
\end{equation}

where

\begin{equation}
    \label{eq:pc_mixture_constraint}
    \left| boldsymbol{a} \right|_2 = 1
\end{equation}

$\boldsymbol{W_c}$ is any vector from $\boldsymbol{\mu}$ to a point on the unit variance ellipsoid. This results in the Mahalanobis distance estimate:

\begin{equation}
    \label{eq:mahalanobis_mixed}
    y = \left| \sum (a_i \mathbf{W_i}) \mathbf{x} - \mathbf{b_c} \right|,
\end{equation}

\subsection{Approximating Mahalanobis Distance with Neural Networks}

Following the Abs function with another linear node allows for a summation of the principal component whitened data, leading to the following expression for the Mahalanobis distance:

\begin{equation}
    \label{eq:mahalanobis_L1}
    y = \sum \left| \mathbf{W_c} \mathbf{x} - \mathbf{b_c} \right|,
\end{equation}

L1 norms have been utilized as an effective alternative to L2 norms in various high-dimensional applications \citep{bernhardsson2018annoy}. In finite-dimensional vector spaces, all norms are equivalent up to constant factors, allowing L1 norms to bound and be bounded by L2 norms \citep{kreyszig1978introductory}. Moreover, L1 norms offer robustness to outliers, making them well-suited for training contexts with unexpected or noisy data \citep{candes2005decoding}. 

In the neural network architecture, each Abs-activated linear node computes the absolute deviation along a specific principal component direction. The subsequent summation node aggregates these deviations, thereby approximating the overall Mahalanobis distance through an L1 norm-based aggregation of L2-like components. This approach leverages the computational efficiency and robustness of L1 norms while maintaining the interpretability of Mahalanobis distance in high-dimensional spaces.
