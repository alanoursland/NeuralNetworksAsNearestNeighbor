% math_framework.tex

\section{Mathematical Framework}
\label{sec:math_framework}

In this section, we develop the mathematical foundation that connects neural networks to the Mahalanobis distance, thereby providing a framework for interpreting neural network operations through the lens of statistical distance metrics. We begin by revisiting key concepts related to Gaussian distributions and the Mahalanobis distance, followed by a detailed exploration of how neural network components, particularly linear layers and activation functions, can approximate these distance metrics. This framework not only enhances our understanding of neural network behavior but also lays the groundwork for leveraging statistical principles to improve network interpretability and training dynamics.

\subsection{Mahalanobis Distance for a Multivariate Gaussian Distribution}

A multivariate Gaussian (Normal) distribution is a fundamental concept in statistics, describing a \(d\)-dimensional random vector \(\mathbf{x} \in \mathbb{R}^d\) with a mean vector \(\boldsymbol{\mu} \in \mathbb{R}^d\) and a covariance matrix \(\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}\) \citep{bishop2006pattern}. We denote this distribution as \(\mathbf{x} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})\).

The Mahalanobis distance quantifies the distance between a point \(\mathbf{x}\) and the mean \(\boldsymbol{\mu}\) of a distribution, while considering the covariance structure of the data \citep{mahalanobis1936generalized, demaesschalck2000mahalanobis}. It is defined as:

\begin{equation}
\label{eq:mahalanobis_distance}
D_M(\mathbf{x}) = \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) }.
\end{equation}

This metric adjusts for variance across dimensions by effectively whitening the data, resulting in a spherical distance measure.

\subsection{Principal Component Analysis (PCA)}

Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a new coordinate system, emphasizing directions (principal components) that capture the most variance \citep{jolliffe2002principal}. When performing PCA on the covariance matrix $\boldsymbol{\Sigma}$, it is decomposed using eigenvalue decomposition:

\begin{equation}
    \label{eq:pca_decomposition}
    \boldsymbol{\Sigma} = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top,
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_d]$ is a matrix whose columns are the orthogonal unit eigenvectors of $\boldsymbol{\Sigma}$.
    \item $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_d)$ is a diagonal matrix of the corresponding eigenvalues $\lambda_i$, representing the variance along each principal component.
\end{itemize}

Substituting $\mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top$ for $\boldsymbol{\Sigma}$ in the Mahalanobis distance equation \eqref{eq:mahalanobis_distance}, we obtain:

\begin{equation}
    \label{eq:mahalanobis_pca}
    D_M(\mathbf{x}) = \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{V} \boldsymbol{\Lambda}^{-1} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}) }.
\end{equation}

To further simplify, we can express the Mahalanobis distance in terms of the principal components:

\begin{align}
D_M(\mathbf{x}) &= \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{V} \boldsymbol{\Lambda}^{-1} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}) } \nonumber \\
&= \sqrt{ (\mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}))^\top \boldsymbol{\Lambda}^{-1} (\mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu})) } \nonumber \\
&= \sqrt{ \sum_{i=1}^{d} \lambda_i^{-1} \left( \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right)^2 } \nonumber \\
&= \left\| \lambda_i^{-1/2} \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right\|_2.
\label{eq:mahalanobis_pca_l2}
\end{align}

where $\| \cdot \|_2$ denotes the Euclidean ($\ell_2$) norm. 

This shows that the Mahalanobis distance can also be expressed as the $\ell_2$ norm of the number of standard deviations of \(\mathbf{x}\) along each principal component.

\subsection{Connecting Neural Networks to Mahalanobis Distance}

We consider the Mahalanobis distance along a single principal component.

\begin{equation}
    \label{eq:mahalanobis_single_component}
    D_{M,i}(\mathbf{x}) = \left| \lambda_i^{-1/2} \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right|,
\end{equation}

This equation projecting the centered data $(\mathbf{x} - \boldsymbol{\mu})$ onto the direction of variance defined by the principal component eigenvector and scales by the inverse square root of the eigenvalue (variance).

Let 
\begin{align}
    \mathbf{W} &= \lambda_i^{-1/2} \mathbf{v}_i^\top, \\
    \mathbf{b} &= - \lambda_i^{-1/2} \mathbf{v}_i^\top \boldsymbol{\mu}.
\end{align}

We can simplify Equation \eqref{eq:mahalanobis_single_component} to

\begin{equation}
    \label{eq:mahalanobis_linear}
    D_{M,i}(\mathbf{x}) = \left| \mathbf{W} \mathbf{x} - \mathbf{b} \right|,
\end{equation}

This is identical to the equation for a linear layer where $\boldsymbol{W}$ represents the weight matrix, $\boldsymbol{b}$ the bias vector, and the Abs function serves as the activation function.

Linear nodes with Abs activations can be interpreted as 1d Gaussians along a direction of variance with the decision boundary passing through the mean of the modelled cluster.

Extending this to all principal components, the Mahalanobis distance can be represented as a multiple linear nodes, each corresponding to a principal component, followed by an Abs activation. This layered structure inherently accounts for the covariance structure of the data, effectively 'whitening' the input.

\subsection{Non-Uniqueness of Whitening}

Calculating the Mahalanobis distance along each principal component results in a whitened data set. 

Whitening data is defined as follows:

\[
\mathbf{x}_w = \boldsymbol{\Lambda}^{-1/2} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}),
\]

where \(\mathbf{x}_w\) follows a distribution \(\mathcal{N}(0, I)\).

The Mahalanobis distance can then be expressed as:

\[
D_M(\mathbf{x}_w) = \|\mathbf{x}_w\|_2.
\]

However the vectors to generate whitened data are not unique.

We can select a rotation matrix \(\mathbf{R} \in SO(n)\) such that:

\begin{align}
    D_M(\mathbf{x}_w) &= D_M(\mathbf{R} \mathbf{x}_w) \\
                      &= D_M\left( (\mathbf{R} \boldsymbol{\Lambda}^{-1/2} \mathbf{V}^\top) (\mathbf{x} - \boldsymbol{\mu}) \right) \\
                      &= D_M\left( \boldsymbol{\Lambda}^{-1/2} (\mathbf{V}^\top \mathbf{R}) (\mathbf{x} - \boldsymbol{\mu}) \right).
    \end{align}
    
The Mahalanobis distance for whitened data remains invariant under the application of rotation matrices. If we transform the rotated basis back to the original space, we end up with a whitening basis whose component vectors may not be orthogonal.

This implies that, although linear nodes can represent principal components, they are unlikely to precisely learn the principal components when estimating Mahalanobis distances. Instead, they are expected to learn a basis that effectively whitens the data. However, the decision boundary of the learned hyperplane should still pass through the data mean.

\subsection{Approximating Mahalanobis Distance with Neural Networks}

In the neural network architecture, each Abs-activated linear node computes the deviation along a specific whitening component. The next linear node sums these values, thereby approximating the overall Mahalanobis distance through an $\ell_1$ norm. 

\begin{equation}
    \label{eq:mahalanobis_L1}
    D_{M,\ell_1}(x) = \sum \left| \mathbf{W_i} \mathbf{x} - \mathbf{b_i} \right|,
\end{equation}

This observation is motivated by a desire to analyze and maintain the behavior of well-studied standard neural network architectures. It offers several additional benefits over $\ell_2$ including computational efficiency and robustness to outliers and sparse data, especially in high dimensional spaces \cite{boyd2004convex, horn2012matrix}.

$\ell_1$ norms have been utilized as an effective alternative to $\ell_2$ norms in various high-dimensional applications \citep{bernhardsson2018annoy, tibshirani1996lasso}. It is well-known that the $\ell_1$ and $\ell_2$ norms are related by the inequality:

\[
\|x\|_2 \leq \|x\|_1 \leq \sqrt{n} \|x\|_2,
\]
where \( x \in \mathbb{R}^n \). 

In high-dimensional spaces, the $\ell_1$ norm is a reasonable approximation of the $\ell_2$ norm \cite{vershynin2018high}. 
