% math_framework.tex

\section{Mathematical Framework}
\label{sec:math_framework}
Gaussians fall out of second-order Taylor series approximations \citep[Section 4.4]{bishop2006pattern}, making them effective for modeling data, even when the data is not explicitly Gaussian. Gaussian mixtures can serve as piecewise linear approximations of complex distributions and surfaces. They are a good choice for modeling point clouds such as the ones neural networks are trained on.

In this section, we develop the mathematical foundation that connects neural networks to the Mahalanobis distance, thereby providing a framework for interpreting neural network operations through the lens of statistical distance metrics. We begin by revisiting key concepts related to Gaussian distributions and the Mahalanobis distance, followed by a detailed exploration of how neural network components, particularly linear layers and activation functions, can approximate these distance metrics. This framework not only enhances our understanding of neural network behavior but also lays the groundwork for leveraging statistical principles to improve network interpretability and training dynamics.

\subsection{Mahalanobis Distance for a Multivariate Gaussian Distribution}

A multivariate Gaussian (Normal) distribution is a fundamental concept in statistics, describing a \(d\)-dimensional random vector \(\mathbf{x} \in \mathbb{R}^d\) with a mean vector \(\boldsymbol{\mu} \in \mathbb{R}^d\) and a covariance matrix \(\boldsymbol{\Sigma} \in \mathbb{R}^{d \times d}\) \citep{bishop2006pattern}. We denote this distribution as \(\mathbf{x} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})\).

The Mahalanobis distance quantifies the distance between a point \(\mathbf{x}\) and the mean \(\boldsymbol{\mu}\) of a distribution, while considering the covariance structure of the data \citep{mahalanobis1936generalized, demaesschalck2000mahalanobis}. It is defined as:

\begin{equation}
\label{eq:mahalanobis_distance}
D_M(\mathbf{x}) = \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) }.
\end{equation}

This metric adjusts for variance across dimensions by effectively whitening the data, resulting in a spherical distance measure.

\subsection{Principal Component Analysis (PCA)}

Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms data into a new coordinate system, emphasizing directions (principal components) that capture the most variance \citep{jolliffe2002principal}. When performing PCA on the covariance matrix $\boldsymbol{\Sigma}$, it is decomposed using eigenvalue decomposition:

\begin{equation}
    \label{eq:pca_decomposition}
    \boldsymbol{\Sigma} = \mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top,
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_d]$ is a matrix whose columns are the orthogonal unit eigenvectors of $\boldsymbol{\Sigma}$.
    \item $\boldsymbol{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_d)$ is a diagonal matrix of the corresponding eigenvalues $\lambda_i$, representing the variance along each principal component.
\end{itemize}

Substituting $\mathbf{V} \boldsymbol{\Lambda} \mathbf{V}^\top$ for $\boldsymbol{\Sigma}$ in the Mahalanobis distance equation \eqref{eq:mahalanobis_distance}, we obtain:

\begin{equation}
    \label{eq:mahalanobis_pca}
    D_M(\mathbf{x}) = \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{V} \boldsymbol{\Lambda}^{-1} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}) }.
\end{equation}

To further simplify, we can express the Mahalanobis distance in terms of the principal components:

\begin{align}
D_M(\mathbf{x}) &= \sqrt{ (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{V} \boldsymbol{\Lambda}^{-1} \mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}) } \nonumber \\
&= \sqrt{ (\mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu}))^\top \boldsymbol{\Lambda}^{-1} (\mathbf{V}^\top (\mathbf{x} - \boldsymbol{\mu})) } \nonumber \\
&= \sqrt{ \sum_{i=1}^{d} \lambda_i^{-1} \left( \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right)^2 } \nonumber \\
&= \left\| \lambda_i^{-1/2} \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right\|_2.
\label{eq:mahalanobis_pca_l2}
\end{align}

where $\| \cdot \|_2$ denotes the Euclidean ($\ell_2$) norm. This shows that the Mahalanobis distance can also be expressed as the $\ell_2$ norm of the number of standard deviations of \(\mathbf{x}\) along each principal component.

\subsection{Connecting Neural Networks to Mahalanobis Distance}

We consider the Mahalanobis distance along a single principal component.

\begin{equation}
    \label{eq:mahalanobis_single_component}
    D_{M,i}(\mathbf{x}) = \left| \lambda_i^{-1/2} \mathbf{v}_i^\top (\mathbf{x} - \boldsymbol{\mu}) \right|,
\end{equation}

This equation projects the centered data $(\mathbf{x} - \boldsymbol{\mu})$ onto the direction of variance defined by the principal component eigenvector and scales it by the inverse square root of the eigenvalue.

Let 
\begin{align}
    \mathbf{W} &= \lambda_i^{-1/2} \mathbf{v}_i^\top, \\
    \mathbf{b} &= - \lambda_i^{-1/2} \mathbf{v}_i^\top \boldsymbol{\mu}.
\end{align}

We can simplify Equation \eqref{eq:mahalanobis_single_component} to

\begin{equation}
    \label{eq:mahalanobis_linear}
    D_{M,i}(\mathbf{x}) = \left| \mathbf{W} \mathbf{x} - \mathbf{b} \right|,
\end{equation}

This is identical to the equation for a linear layer where $\boldsymbol{W}$ represents the weight matrix, $\boldsymbol{b}$ the bias vector, and the Abs function serves as the activation function. Linear nodes with Abs activations can be interpreted as 1d Gaussians along a direction of variance with the decision boundary passing through the mean of the modelled cluster. 

Extending this to all principal components, the Mahalanobis distance can be represented as a multiple linear nodes, each corresponding to a principal component, followed by an Abs activation. This layered structure inherently accounts for the covariance structure of the data, effectively 'whitening' the input.

This is identical to the equation for a linear layer where 
$\boldsymbol{W}$
W represents the weight matrix, 
$\boldsymbol{b}$
b the bias vector, and the Abs function serves as the activation function. Each linear node with an Abs activation can be interpreted as modeling a one-dimensional Gaussian along a principal component direction, with the decision boundary passing through the mean of the modeled cluster. The layer as a whole represents a subset of principal components from a Gaussian Mixture Model (GMM) that approximates the input distribution. Since each component captures significant features individually, we do not need to aggregate them via an $\ell_2$ norm (full Mahalanobis distance computation). Instead, the subsequent layer clusters these principal component features, effectively forming a new GMM that models the outputs of the first layer.

\subsection{Non-Uniqueness of Whitening}

The principal components of a Gaussian distribution, as used in the Mahalanobis distance, form an orthonormal set of axes. Projecting Gaussian data onto these axes transforms the distribution from an oriented ellipsoid into a spherical Gaussian, effectively converting the data from \(\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\) to \(\mathcal{N}(\mathbf{0}, \mathbf{I})\). This process is known as \emph{whitening}.

However, the transformation to whitened data is not unique. Specifically, any rotation applied in the whitened space results in another valid whitening transformation. Mathematically, if \(\mathbf{x}_w\) is the whitened data, then for any orthogonal rotation matrix \(\mathbf{R} \in \text{SO}(d)\), the rotated data \(\mathbf{x}_w' = \mathbf{R} \mathbf{x}_w\) is also whitened, since:

\begin{align}
\operatorname{Cov}(\mathbf{x}_w') &= \operatorname{Cov}(\mathbf{R} \mathbf{x}_w) = \mathbf{R} \operatorname{Cov}(\mathbf{x}_w) \mathbf{R}^\top \\
&= \mathbf{R} \mathbf{I} \mathbf{R}^\top = \mathbf{R} \mathbf{R}^\top = \mathbf{I}.
\end{align}

This non-uniqueness implies that multiple sets of axes, possibly non-orthogonal in the original space, can serve as a whitening basis. When we transform the rotated basis back to the original space, we obtain a new set of basis vectors \(\mathbf{W}\) that still whiten the data but may not correspond to the original principal components.

In the context of neural networks, this means that although linear nodes can represent directions that effectively whiten the data, they are unlikely to precisely learn the actual principal components when estimating Mahalanobis distances. Instead, they may learn any basis that achieves whitening. Nevertheless, the learned hyperplanes (decision boundaries) should still pass through the data mean \(\boldsymbol{\mu}\), allowing for prototype interpretation.

To encourage the network to learn the actual principal components, one could apply an orthogonality constraint or regularization on the weight matrices. This regularization promotes learning orthogonal directions, aligning the learned basis with the true principal components of the data clusters and providing statistically independent features.
