% introduction.tex

\section{Introduction}

Neural networks have revolutionized the field of machine learning, achieving remarkable success across diverse applications such as image recognition, natural language processing, and autonomous driving. Central to their efficacy is the use of activation functions, which introduce non-linearity into the network, enabling the modeling of complex relationships within data. From the early adoption of sigmoid and hyperbolic tangent (Tanh) functions to the widespread use of Rectified Linear Units (ReLU), the evolution of activation functions has been pivotal in addressing challenges such as the vanishing gradient problem \citep{glorot2010understanding, he2015delving}. ReLU, in particular, has gained prominence due to its simplicity and effectiveness in promoting sparse activations, which facilitate faster training and mitigate issues related to gradient saturation \citep{nair2010rectified}.

Despite the success of ReLU, the exploration of alternative activation functions remains an open and valuable area of research. The choice of activation function significantly influences the network's ability to learn and generalize from data, prompting ongoing investigations into functions that can offer improved performance, stability, and interpretability \citep{ramachandran2017searching}. This paper delves into the mathematical underpinnings of neural networks by establishing a connection between neural network architectures and distance metrics, specifically the Mahalanobis distance. By interpreting neural networks through the lens of Mahalanobis distance, we aim to enhance the interpretability of neural network models and provide a robust mathematical framework that bridges neural networks with statistical distance measures.

Traditional neural network training paradigms often rely on loss functions that interpret larger activation values as stronger signals indicative of class membership. For instance, Cross Entropy Loss \citep{bishop1995neural} assumes that higher output values correspond to higher confidence in class predictions. Similarly, Hinge Loss \citep{cortes1995support} discourages small activation values, promoting maximal separation between classes by penalizing points that lie close to the decision boundary. This perspective aligns with the interpretation of linear layers as linear separators that aim to produce large activation magnitudes to clearly distinguish between different classes.

In contrast, distance metrics like the Mahalanobis distance operate under a different paradigm where smaller distance values signify closer proximity to a feature or class center. Instead of enforcing linear separability, distance-based approaches inherently perform clustering by grouping data points based on their proximity in a transformed feature space. This fundamental difference raises intriguing questions about the potential for neural networks to incorporate distance-based interpretations within their architecture and training dynamics.

Neural networks, particularly their internal layers, exhibit versatile behaviors that can align with either separation-based or distance-based interpretations. The flexibility of neural networks allows subsequent layers to negate or adjust node outputs, effectively transforming large positive activations into large negative ones, and vice versa. This adaptability suggests that neural networks possess the inherent capacity to balance between maximizing separation and performing clustering operations, depending on the activation functions and loss functions employed.

The magnitude of activation values plays a critical role in the training dynamics of neural networks. Large activation magnitudes can lead to substantial gradient updates, resulting in significant changes to the network's weights. Conversely, smaller activation magnitudes contribute to more stable and gradual weight updates, potentially enhancing the training stability \citep{lecun2012efficient}. Distance metrics, by emphasizing smaller values for stronger signals, may inherently promote more stable training processes by encouraging consistent and incremental weight adjustments.

Motivated by these observations, this paper presents a mathematical framework that connects linear units in neural networks with Gaussian distributions through the Mahalanobis distance. We demonstrate that linear layers equipped with absolute value (Abs) activation functions can approximate the Mahalanobis distance, thereby enabling neural networks to perform distance-based clustering operations. This theoretical connection not only provides a new perspective on neural network interpretability but also suggests potential modifications to network architectures that can enhance feature learning and model stability.

Furthermore, we explore how ReLU activation functions, despite their inherent asymmetry, can represent similar distance-based information through architectural adaptations. By leveraging multiple ReLU units or adjusting network configurations, ReLU-based networks can approximate the behavior of Abs-activated networks, offering a pathway to integrate distance-based interpretations within existing neural network frameworks.

The contributions of this paper are threefold:
\begin{enumerate}
    \item We establish a mathematical connection between neural network linear layers and the Mahalanobis distance, demonstrating how Abs activations facilitate distance-based interpretations.
    \item We analyze the implications of activation function choices on neural network behavior, highlighting how Abs and ReLU activations influence feature learning and training dynamics.
    \item We discuss the broader implications of this framework for neural network design, interpretability, and future research directions, laying the groundwork for more interpretable and stable neural network models.
\end{enumerate}

By bridging the gap between neural networks and statistical distance measures, this work contributes to a deeper understanding of neural network mechanisms and opens avenues for developing more interpretable and robust machine learning models. The ensuing sections elaborate on the mathematical framework, explore distance approximators within neural networks, discuss the implications for network design and training, and outline potential extensions and future research directions.

