% background.tex
\section{Background and Related Work}

\subsection{Activation Functions}

Activation functions introduce non-linearity in neural networks, enabling them to model complex data relationships. The field has evolved from early sigmoid and hyperbolic tangent functions \citep{rosenblatt1958perceptron} to the widely adopted Rectified Linear Unit (ReLU) \citep{nair2010rectified}, which mitigates the vanishing gradient problem in deep networks \citep{glorot2010understanding, krizhevsky2012imagenet}.

ReLU variants intended to address its shortcomings include Leaky ReLU \citep{maas2013rectifier}; Parametric ReLU (PReLU) \citep{he2015delving}; and Exponential Linear Unit (ELU) \citep{clevert2015fast}. Additionally, newer activation functions like Swish \citep{ramachandran2017searching} and GELU \citep{hendrycks2016gaussian} have been proposed to further enhance network performance and training dynamics.

Tanh and Sigmoid activations are still used in many architectures such as recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks \citep{hochreiter1997long}.

The variety of activation functions used in modern networks reflects the diverse needs of different architectures. The exploration of activation functions remains an active area of research, with ongoing investigations into their impact on neural network performance, generalization, and interpretability \citep{ramachandran2017searching}. Despite extensive research, the interpretation of activation functions in terms of statistical measures remains an open area of investigation.

\subsection{Overview of Distance Metrics in Clustering}

Distance metrics are fundamental in clustering algorithms, determining how similarity between data points is measured. Various clustering methods employ different distance measures:

\begin{itemize}
    \item K-Means typically uses Euclidean distance ($\ell_2$ norm), assuming spherical clusters and equal feature importance \citep{macqueen1967some}.
    
    \item Gaussian Mixture Models (GMMs) employ Mahalanobis distance, accounting for data covariance and modeling elliptical clusters \citep{reynolds2009gaussian}.

    \item Radial Basis Function (RBF) networks use Gaussian-like activations based on Euclidean distance, creating spherical clusters around learned centers \citep{broomhead1988radial}.

    \item Hierarchical and Agglomerative Clustering can use various metrics (Euclidean, Manhattan, correlation-based), affecting dendrogram shape \citep{murtagh1983survey}.
    
    \item DBSCAN, while often using Euclidean distance, can employ any metric for density-based clustering \citep{ester1996density}.
    
    \item Spectral Clustering incorporates similarity measures like Gaussian kernel functions \citep{von2007tutorial}.
\end{itemize}

The Mahalanobis distance stands out for its ability to account for feature correlations and scale differences, making it particularly useful in multivariate analysis \citep{mahalanobis1936generalized, demaesschalck2000mahalanobis}. It provides a scale-invariant measure that adjusts for the covariance structure of the data, offering advantages in high-dimensional spaces.

Understanding these distance metrics and their properties is crucial for selecting appropriate clustering algorithms and interpreting their results. As we explore the connection between neural networks and distance-based interpretations, these insights from clustering algorithms provide valuable context and inspiration.

\subsection{Neural Network Interpretability and Statistical Models}

The interpretability of neural networks remains a critical challenge, often referred to as the "black-box" nature of these models \citep{lipton2016mythos}. In applications requiring transparency, such as healthcare and finance, understanding the decision-making processes of neural networks is paramount \citep{rudin2019stop}. Various approaches have been developed to enhance interpretability, including feature visualization, saliency maps, and prototype-based methods \citep{erhan2009visualizing, simonyan2013deep, kim2016interpreting}.

Recent advancements in explainable AI (XAI) have introduced tools like SHAP (SHapley Additive exPlanations) \citep{lundberg2017unified} and LIME (Local Interpretable Model-Agnostic Explanations) \citep{ribeiro2016should}, which provide both local and global insights into model predictions. SHAP offers a consistent approach to feature attribution grounded in cooperative game theory, providing robust explanations even for complex models like deep neural networks \citep{lundberg2017unified,markov2020shap}. LIME, by contrast, offers localized interpretability by approximating the modelâ€™s behavior with simpler, interpretable models, making it particularly useful for individual predictions \citep{ribeiro2016should,ali2024explainable}.

Connections between neural networks and statistical models, such as Bayesian neural networks \citep{neal1996bayesian,blundell2015weight}, continue to provide a probabilistic framework for understanding model uncertainty. Moreover, concept-based interpretability methods like TCAV (Testing with Concept Activation Vectors) \citep{kim2018interpretability} have emerged, allowing for a more granular analysis of what neural networks learn, which can be crucial in high-stakes domains like healthcare \citep{hanif2024systematic}.

While significant strides have been made in explaining model behavior, there remains a gap in establishing direct mathematical connections between neural network components, specifically activation functions, and statistical distance measures like the Mahalanobis distance. Addressing this gap can provide deeper insights into feature learning and decision-making processes, enhancing both interpretability and robustness of neural network models.
