% introduction.tex

\section{Introduction}

Neural networks have become a cornerstone of modern machine learning, achieving state-of-the-art results in various domains. Despite their success, understanding the internal workings and interpretability of neural networks remains a challenge. In this paper, we propose a novel interpretation by viewing neural networks as nearest neighbor models.

We begin by deriving mathematical connections between linear layers and Mahalanobis distances. Building on this foundation, we conduct experiments to validate our approach and compare it with traditional activation functions and nearest neighbor algorithms.

Our contributions are as follows:
\begin{itemize}
    \item We provide a mathematical framework linking neural networks to nearest neighbor methods.
    \item We demonstrate through experiments how linear nodes with absolute value activations approximate distances to cluster centers.
    \item We interpret the learned features of neural networks under this new perspective.
\end{itemize}

% The remainder of this paper is organized as follows: Section~\ref{sec:math_framework} presents the mathematical framework. Section~\ref{sec:experiment1} details the first experiment on a synthetic dataset. Section~\ref{sec:experiment2} explores experiments on the MNIST dataset. In Section~\ref{sec:interpretation}, we interpret the learned features. Section~\ref{sec:experiment3} applies the approach to the LeNet architecture. Finally, Section~\ref{sec:discussion} discusses the implications, and Section~\ref{sec:conclusion} concludes the paper.




