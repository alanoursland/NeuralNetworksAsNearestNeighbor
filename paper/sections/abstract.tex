% abstract.tex

In this paper, we present a new perspective on neural networks by interpreting them as nearest neighbor models. We demonstrate mathematically how linear layers with absolute value activations approximate Mahalanobis distances to cluster centers. Through experiments on synthetic data and the MNIST dataset, we compare our approach with traditional ReLU networks and nearest neighbor algorithms. Our findings offer insights into neural network interpretability and feature learning.

